---
title: "PPHA 30538 1- Final Project Writeup"
author: "Xinyi Zhou, and Wuzhen Han"
date: 12/5/2024
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
    documentclass: article
    keep-tex: true
    number-sections: true
    pdf-engine: lualatex
    geometry: margin=0.3in
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

Group Members: 

Xinyi Zhou (Github:XinyiZhou66) , Responsible part: line chart, and shiny app part 2 (Trends in Real GDP and Unemployment Rate… )

Wuzhen Han (Github:hanwuzhen), Responsible part: bar chart, and shiny app part 1 (Unemployment Rate by State- heatmap)


# Research Topic: Evaluating the Effectiveness of the CARES Act in Accelerating Employment Recovery During the COVID-19 Pandemic

The goal of this research is to evaluate the effectiveness of the CARES Act in accelerating employment recovery during the COVID-19 pandemic. Specifically, we analyzed trends in Real Gross Domestic Product (GDP) and unemployment rates to determine if the CARES Act, with its economic stimulus provisions, helped reduce unemployment more effectively compared to other measures. Our analysis took place at both the national and state levels, focusing on disparities across states and changes over time to assess the Act’s impact comprehensively.

To conduct our analysis, we utilized multiple data sources. The GDP data was collected from the Federal Reserve Economic Data (FRED) database, which provided an accurate representation of the U.S. economy's growth over time. For unemployment rates, we used two datasets from the Bureau of Labor Statistics (BLS). The first dataset provided overall yearly unemployment rates for the entire United States, which we downloaded directly from the BLS website. The second dataset contained state-level unemployment rates. Additionally, information on CARES Act funding was sourced from Treasury Department documents, and geographic data were collected from the U.S. Census Bureau, which helped facilitate our state-level analysis.

We conducted Natural Language Processing (NLP) analysis on the CARES Act report to gain insights. Sentiment analysis helped assess the emotional tone, capturing polarity and subjectivity. Named Entity Recognition identified key entities like organizations and locations, while tokenization and Part-of-Speech tagging provided a detailed breakdown of the text. This allowed us to better understand the content and overall narrative of the document.

Our analysis relies on two key assumptions. First, we assume that changes in unemployment rates and Real GDP are largely due to the CARES Act, though other factors may also influence these outcomes. We also assume uniform state responses to the CARES Act, ignoring differences in local policies and economic conditions. 

The research employed quantitative, descriptive, and interactive analysis to evaluate the impact of the CARES Act. Quantitative analysis included static visualizations such as bar charts depicting the distribution of CARES Act funding and line plots of unemployment rates and Real GDP trends. California, Texas, and New York received the most funding, and we compared this funding distribution to unemployment trends to assess recovery. Interactive visualizations provided deeper insight into economic changes, aiming to offer both macro and detailed perspectives on the CARES Act's impact during the pandemic. This dashboard had two main components:

The first component was a heatmap showing state-level unemployment rates from 2018 to 2024. Users could select different years to observe how unemployment varied across the United States, highlighting the three states with the highest rates each year. The second component of our dashboard focused on broader trends in economic indicators such as Real GDP and unemployment rates over the same period. This component allowed users to explore different time periods, such as before the CARES Act was implemented (2018 Q1 to 2020 Q1), during the CARES Act implementation (2020 Q2), and after its implementation. These periods helped illustrate changes over time, such as the initial surge in unemployment and GDP drop at the start of the pandemic and the subsequent recovery. The line charts clearly depicted the impact of the CARES Act's timing on both employment and GDP recovery.

The analysis led to several notable findings. In 2019, unemployment rates were relatively low. However, in 2020, unemployment surged, especially in tourism-heavy states like Nevada, Hawaii, and California, with Nevada reaching 13.5%. By 2021, unemployment rates declined, with California dropping from 10.1% in 2020 to 7.3%, partly influenced by the $193 million it received from the CARES Act. This suggests that targeted federal aid may have helped mitigate the effects of unemployment in hard-hit states. Nationally, a vertical dashed line marked the CARES Act implementation, helping to visualize its impact. The trend analysis revealed an economic downturn in early 2020, with a spike in unemployment and a GDP drop. Following the CARES Act, there was an almost immediate recovery in GDP and unemployment. The interactive dashboard helped users distinguish short-term improvements from longer-term trends, showing continued economic recovery. These findings suggest that the CARES Act was instrumental in stabilizing the economy and aiding employment recovery, though other factors also contributed.

In our project, one limitation was that we did not account for population differences when analyzing the distribution of CARES Act funding. The bar plot presented the total funding received by each state, meaning that larger states like California and Texas naturally appeared to receive more. This approach lacks a per capita perspective, which would have provided more insight into the fairness and effectiveness of the fund allocation relative to each state’s population and needs. Additionally, our analysis focused primarily on unemployment rate and Real GDP, while neglecting other important metrics like inflation rate or household financial stability, which could have offered a broader understanding of the CARES Act's impact during the pandemic.

For future work, incorporating econometric models could better help isolate the impact of the CARES Act from other influencing factors. This would improve the robustness of our conclusions. Moreover, breaking down the "Unemployment Rate by State" into monthly data for 2021 could provide insights into how different cities were impacted over time, capturing the dynamic response in key metropolitan areas. This approach would allow us to identify whether some cities recovered faster than others and understand the differential effects within states, offering a more granular perspective on how effectively the CARES Act supported various local economies. It would also be valuable to explore other aspects, such as household financial stability or small business recovery, to gain a broader understanding of the CARES Act's influence during the pandemic.


```{python}
# | include: false

import pandas as pd
import altair as alt
import geopandas as gpd
import json
import os
from vega_datasets import data
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PyPDF2 import PdfReader
import spacy
from spacytextblob.spacytextblob import SpacyTextBlob

import warnings
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```

```{python}
#| echo: false

# What is CARES Act
# data was from "https://home.treasury.gov/system/files/136/Two-Year-ARP-Anniversary-Report.pdf"
pdf_path = '/Users/cynthia/Desktop/final-project-xy-wz/data/Extra credit.pdf'
output_path = '/Users/cynthia/Desktop/final-project-xy-wz/data/nlp_analysis.txt'

reader = PdfReader(pdf_path)
text = ""
for page in reader.pages:
    text += page.extract_text()

try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    import os
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

nlp.add_pipe('spacytextblob')

doc = nlp(text)

with open(output_path, 'w', encoding='utf-8') as f:
    f.write("\n=== Sentiment Analysis ===\n")
    f.write(f"Overall Polarity: {doc._.blob.polarity}\n")
    f.write(f"Overall Subjectivity: {doc._.blob.subjectivity}\n")

    f.write("\nSentence-level Sentiment Analysis:\n")
    for sent in doc.sents:
        f.write(f"Sentence: {sent.text}\n")
        f.write(f"Polarity: {sent._.blob.polarity}\n")
        f.write(f"Subjectivity: {sent._.blob.subjectivity}\n")
        f.write("\n")

    f.write("\n=== Named Entity Recognition (NER) ===\n")
    for ent in doc.ents:
        f.write(f"Entity: {ent.text}, Label: {ent.label_}\n")

    f.write("\n=== Tokenization and POS tagging ===\n")
    for token in doc[:20]:
        f.write(f"Token: {token.text}, POS: {token.pos_}\n")

    f.write("\n=== Sentences ===\n")
    sentences = list(doc.sents)
    for sent in sentences[:5]:
        f.write(f"{sent}\n")


```


```{python}
#| echo: false

# Merge Data 
data_path = '/Users/cynthia/Desktop/final-project-xy-wz/data'
real_gdp_path = os.path.join(data_path, 'Real_GDP.csv')
unemployment_rate_path = os.path.join(data_path, 'Unemployment_rate.csv')

real_gdp = pd.read_csv(real_gdp_path)
unemployment_rate = pd.read_csv(unemployment_rate_path)

real_gdp['DATE'] = pd.to_datetime(real_gdp['DATE'])
unemployment_rate['DATE'] = pd.to_datetime(unemployment_rate['DATE'])

# Merge the datasets on the DATE column, keeping all rows from real_gdp left join
merged_data = pd.merge(real_gdp, unemployment_rate, on='DATE', how='left')

save_path = os.path.join(data_path, 'merged_data.csv')
merged_data.to_csv(save_path, index=False)

```

```{python}
#| echo: false

covid_report_path = os.path.join(data_path, 'COVID19_Grant_Report.csv')
data_raw = pd.read_csv(covid_report_path, skiprows=5)

original_number_of_grants = len(data_raw)
# Remove dollar signs and commas from Award Funding and convert to float
data_raw['Award Funding'] = data_raw['Award Funding'].replace(
    r'[$,]', '', regex=True).astype(float)

converted_number_of_grants = len(data_raw)

data_cleaned = data_raw.dropna(subset=['State', 'Award Funding'])

total_funding_dollars = data_cleaned['Award Funding'].sum()

# Group by State and sum the Award Funding, converting to millions
state_funding = data_cleaned.groupby(
    'State')['Award Funding'].sum().reset_index()

state_funding['Award Funding'] = state_funding['Award Funding'] / 1e6

# Sort states by funding amount in descending order
state_funding = state_funding.sort_values(
    by='Award Funding', ascending=False).reset_index(drop=True)

# Bar chart of state-level CARES Act funding distribution
chart = alt.Chart(state_funding).mark_bar(color='skyblue').encode(
    y=alt.Y('State:N', sort='-x', title='State'),
    x=alt.X('Award Funding:Q', title='Total Award Funding (Millions $)'),
    tooltip=['State', 'Award Funding']
).properties(
    title='Total Funding Amount by State',
    width=500,
    height=250
).configure_axis(
    labelAngle=0
)

chart

```
 

```{python}
#| echo: false

import pandas as pd
import altair as alt

# Create a line chart for Real GDP
gdp_chart = alt.Chart(merged_data).mark_line(color='blue').encode(
    alt.X('DATE:T', title='Date', axis=alt.Axis(format='%Y/%m/%d', grid=False)),
    alt.Y('GDPC1:Q', title='Real GDP (Billion $)', axis=alt.Axis(grid=False),
          scale=alt.Scale(domain=[merged_data['GDPC1'].min(), merged_data['GDPC1'].max()]))
)

# Create a line chart for Unemployment Rate
unrate_chart = alt.Chart(merged_data).mark_line(color='orange').encode(
    alt.X('DATE:T', title='Date', axis=alt.Axis(format='%Y/%m/%d', grid=False)),
    alt.Y('UNRATE:Q', title='Unemployment Rate (%)', axis=alt.Axis(grid=False),
          scale=alt.Scale(domain=[0, merged_data['UNRATE'].max()]))
).properties(
    title="Time Trends: Real GDP and Unemployment Rate (2018–2024)"
)

# Add a dashed vertical line on March 27, 2020 (CARES Act implementation date)
vertical_line = alt.Chart(pd.DataFrame({'DATE': ['2020-03-27']})).mark_rule(
    color='red', strokeDash=[6, 3], strokeWidth=2
).encode(
    x=alt.X('DATE:T')
)

# Add text label for CARES Act
text_label = alt.Chart(pd.DataFrame({'DATE': ['2020-03-27'], 'label': ['CARES Act Begins: 2020-03-27']})).mark_text(
    align='left',
    baseline='middle',
    dx=5, 
    dy=-190,  
    color='red'
).encode(
    x=alt.X('DATE:T'),
    text='label'
)

# 2. Economic Trends During COVID-19: Real GDP and Unemployment Rate (2018–2024)
chart = alt.layer(
    gdp_chart, unrate_chart, vertical_line, text_label
).resolve_scale(
    y='independent'
).properties(
    width=500,
    height=250
)

chart.show()


```


```{python}
#| echo: false

# 3. State-Level Unemployment Rate: Dynamic Heatmap by Time

# getting data
# Download the required unemployment rate documentation（https://dlt.ri.gov/media/15101/download?language=en）.It is then processed manually to extract key information from the file

# a. Create static maps of unemployment rates by state

# 2024
unemp_state = os.path.join(data_path, 'anunemp.csv')
unemployment_data = pd.read_csv(unemp_state)

unemployment_data_filtered = unemployment_data.loc[unemployment_data['State'] != "United States"].copy(
)
unemployment_data_filtered['State'] = unemployment_data_filtered['State'].str.strip(
).str.title()

data_path_shp = '/Users/cynthia/Desktop/final-project-xy-wz/data/cb_2018_us_state_500k'
shapefile_path = os.path.join(data_path_shp, 'cb_2018_us_state_500k.shp')
gdf_states = gpd.read_file(shapefile_path)

gdf_states['NAME'] = gdf_states['NAME'].str.strip().str.title()

gdf_merged = gdf_states.merge(
    unemployment_data_filtered, how='left', left_on='NAME', right_on='State'
)
gdf_merged['Rate_2024'] = gdf_merged['Rate_2024'].fillna(0)

geojson = gdf_merged.to_json()
geojson_data = json.loads(geojson)

states = alt.Data(values=geojson_data['features'])

unemployment_chart_2024 = alt.Chart(states).mark_geoshape(
    stroke='black',
    strokeWidth=1
).encode(
    color=alt.Color('properties.Rate_2024:Q',
                    scale=alt.Scale(domain=[0, 10], scheme='orangered'),
                    title='Unemployment Rate in 2024 (%)'),
    tooltip=[
        alt.Tooltip('properties.NAME:N', title='State'),
        alt.Tooltip('properties.Rate_2024:Q', title='Unemployment Rate (%)')
    ]
).project(
    type='albersUsa'
).properties(
    width=400,
    height=300,
    title='Unemployment Rate by State (2024)'
)

```

```{python}
#| echo: false
# 2023

gdf_merged_2023 = gdf_states.merge(
    unemployment_data_filtered, how='left', left_on='NAME', right_on='State'
)
gdf_merged_2023['Rate_2023'] = gdf_merged_2023['Rate_2023'].fillna(0)

geojson = gdf_merged_2023.to_json()
geojson_data = json.loads(geojson)

states = alt.Data(values=geojson_data['features'])

unemployment_chart_2023 = alt.Chart(states).mark_geoshape(
    stroke='black',
    strokeWidth=1
).encode(
    color=alt.Color('properties.Rate_2023:Q',
                    scale=alt.Scale(domain=[0, 10], scheme='orangered'),
                    title='Unemployment Rate in 2023 (%)'),
    tooltip=[
        alt.Tooltip('properties.NAME:N', title='State'),
        alt.Tooltip('properties.Rate_2023:Q', title='Unemployment Rate (%)')
    ]
).project(
    type='albersUsa'
).properties(
    width=400,
    height=300,
    title='Unemployment Rate by State (2023)'
)

```


```{python}
#| echo: false
# 2022

gdf_merged_2022 = gdf_states.merge(
    unemployment_data_filtered, how='left', left_on='NAME', right_on='State'
)
gdf_merged_2022['Rate_2022'] = gdf_merged_2022['Rate_2022'].fillna(0)

geojson_2022 = gdf_merged_2022.to_json()
geojson_data_2022 = json.loads(geojson_2022)

states_2022 = alt.Data(values=geojson_data_2022['features'])

unemployment_chart_2022 = alt.Chart(states_2022).mark_geoshape(
    stroke='black',
    strokeWidth=1
).encode(
    color=alt.Color('properties.Rate_2022:Q', 
                    scale=alt.Scale(domain=[0, 10], scheme='orangered'),
                    title='Unemployment Rate in 2022 (%)'),
    tooltip=[
        alt.Tooltip('properties.NAME:N', title='State'),
        alt.Tooltip('properties.Rate_2022:Q', title='Unemployment Rate (%)')
    ]
).project(
    type='albersUsa'  
).properties(
    width=400,
    height=300,
    title='Unemployment Rate by State (2022)'
)

```

```{python}
#| echo: false
# 2021

gdf_merged_2021 = gdf_states.merge(
    unemployment_data_filtered, how='left', left_on='NAME', right_on='State')
gdf_merged_2021['Rate_2021'] = gdf_merged_2021['Rate_2021'].fillna(0)

geojson_2021 = gdf_merged_2021.to_json()
geojson_data_2021 = json.loads(geojson_2021)

states_2021 = alt.Data(values=geojson_data_2021['features'])

unemployment_chart_2021 = alt.Chart(states_2021).mark_geoshape(
    stroke='black',
    strokeWidth=1
).encode(
    color=alt.Color('properties.Rate_2021:Q',
                    scale=alt.Scale(domain=[0, 10], scheme='orangered'),
                    title='Unemployment Rate in 2021 (%)'),
    tooltip=[
        alt.Tooltip('properties.NAME:N', title='State'),
        alt.Tooltip('properties.Rate_2021:Q', title='Unemployment Rate (%)')
    ]
).project(
    type='albersUsa'
).properties(
    width=400,
    height=300,
    title='Unemployment Rate by State (2021)'
)

```


```{python}
#| echo: false
# 2020，2019，2018

gdf_merged_2020 = gdf_states.merge(
    unemployment_data_filtered, how='left', left_on='NAME', right_on='State').copy()
gdf_merged_2020['Rate_2020'] = gdf_merged_2020['Rate_2020'].fillna(0)

geojson_2020 = gdf_merged_2020.to_json()
geojson_data_2020 = json.loads(geojson_2020)

states_2020 = alt.Data(values=geojson_data_2020['features'])

unemployment_chart_2020 = alt.Chart(states_2020).mark_geoshape(
    stroke='black',
    strokeWidth=1
).encode(
    color=alt.Color('properties.Rate_2020:Q',
                    scale=alt.Scale(domain=[0, 10], scheme='orangered'),
                    title='Unemployment Rate in 2020 (%)'),
    tooltip=[
        alt.Tooltip('properties.NAME:N', title='State'),
        alt.Tooltip('properties.Rate_2020:Q', title='Unemployment Rate (%)')
    ]
).project(
    type='albersUsa'
).properties(
    width=400,
    height=300,
    title='Unemployment Rate by State (2020)'
)

top_3_states_2020 = gdf_merged_2020.nlargest(
    3, 'Rate_2020')[['NAME', 'Rate_2020']]

gdf_merged_2019 = gdf_states.merge(
    unemployment_data_filtered, how='left', left_on='NAME', right_on='State').copy()
gdf_merged_2019['Rate_2019'] = gdf_merged_2019['Rate_2019'].fillna(0)

geojson_2019 = gdf_merged_2019.to_json()
geojson_data_2019 = json.loads(geojson_2019)

states_2019 = alt.Data(values=geojson_data_2019['features'])

unemployment_chart_2019 = alt.Chart(states_2019).mark_geoshape(
    stroke='black',
    strokeWidth=1
).encode(
    color=alt.Color('properties.Rate_2019:Q',
                    scale=alt.Scale(domain=[0, 10], scheme='orangered'),
                    title='Unemployment Rate in 2019 (%)'),
    tooltip=[
        alt.Tooltip('properties.NAME:N', title='State'),
        alt.Tooltip('properties.Rate_2019:Q', title='Unemployment Rate (%)')
    ]
).project(
    type='albersUsa'
).properties(
    width=400,
    height=300,
    title='Unemployment Rate by State (2019)'
)

gdf_merged_2018 = gdf_states.merge(
    unemployment_data_filtered, how='left', left_on='NAME', right_on='State').copy()
gdf_merged_2018['Rate_2018'] = gdf_merged_2018['Rate_2018'].fillna(0)

geojson_2018 = gdf_merged_2018.to_json()
geojson_data_2018 = json.loads(geojson_2018)

states_2018 = alt.Data(values=geojson_data_2018['features'])

unemployment_chart_2018 = alt.Chart(states_2018).mark_geoshape(
    stroke='black',
    strokeWidth=1
).encode(
    color=alt.Color('properties.Rate_2018:Q',
                    scale=alt.Scale(domain=[0, 10], scheme='orangered'),
                    title='Unemployment Rate in 2018 (%)'),
    tooltip=[
        alt.Tooltip('properties.NAME:N', title='State'),
        alt.Tooltip('properties.Rate_2018:Q', title='Unemployment Rate (%)')
    ]
).project(
    type='albersUsa'
).properties(
    width=400,
    height=300,
    title='Unemployment Rate by State (2018)'
)

```

```{python}
# | include: false

output_path = '/Users/cynthia/Desktop/final-project-xy-wz/write_up/pictures'

unemployment_chart_2018.save(os.path.join(
    output_path, 'unemployment_rate_2018.png'))
unemployment_chart_2019.save(os.path.join(
    output_path, 'unemployment_rate_2019.png'))
unemployment_chart_2020.save(os.path.join(
    output_path, 'unemployment_rate_2020.png'))
unemployment_chart_2021.save(os.path.join(
    output_path, 'unemployment_rate_2021.png'))
unemployment_chart_2022.save(os.path.join(
    output_path, 'unemployment_rate_2022.png'))
unemployment_chart_2023.save(os.path.join(
    output_path, 'unemployment_rate_2023.png'))
unemployment_chart_2024.save(os.path.join(
    output_path, 'unemployment_rate_2024.png'))


```



```{python}
# | include: false
# b. Create the dynamic maps of unemployment rates by state in app.py file
# For a detailed view of how these figures were generated, please refer to the code in app.py. The dynamic trends presented here are the final output of our detailed data processing and visualization pipeline. 
dynamic_1_path = '/Users/cynthia/Desktop/final-project-xy-wz/shiny-app/picture/dynamic_1'

image_path_1 = os.path.join(dynamic_1_path, 'p1.png')

img_1 = mpimg.imread(image_path_1)
plt.figure(figsize=(9, 6)) 
plt.imshow(img_1)
plt.axis('off')
plt.show()
```

```{python}
# | include: false

image_path_2 = os.path.join(dynamic_1_path, 'p2.png')

img_2 = mpimg.imread(image_path_2)
plt.figure(figsize=(9, 6))
plt.imshow(img_2)
plt.axis('off')
plt.show()

```

```{python}
#| echo: false

image_path_3 = os.path.join(dynamic_1_path, 'p3.png')

img_3 = mpimg.imread(image_path_3)
plt.figure(figsize=(9, 6))
plt.imshow(img_3)
plt.axis('off')
plt.show()

```



```{python}
#| echo: false

# 4. Dynamic Trends of Real GDP and Unemployment Rate Under CARES Act 
# For a detailed view of how these figures were generated, please refer to the code in app.py. The dynamic trends presented here are the final output of our detailed data processing and visualization pipeline. 

dynamic_2_path = '/Users/cynthia/Desktop/final-project-xy-wz/shiny-app/picture/dynamic_2'

screenshot_path_1 = os.path.join(dynamic_2_path, 'All_period.png')
screenshot_path_2 = os.path.join(dynamic_2_path, 'Pre_Cares.png')
screenshot_path_3 = os.path.join(dynamic_2_path, 'Implementation_period.png')
screenshot_path_4 = os.path.join(dynamic_2_path, 'After_Implementation.png') 


def show_image(image_path, figsize=(10, 20)):
    img = mpimg.imread(image_path)
    plt.figure(figsize=figsize)
    plt.imshow(img)
    plt.axis('off')
    plt.show()

# Show all the images sequentially to highlight different phases under CARES Act
show_image(screenshot_path_3)
show_image(screenshot_path_4)


```
